{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Launching a corporate documentation management service using MediaWiki Dmitry Kirsanov | GitHub The project involves the deployment of a corporate documentation management service using the MediaWiki application. Initial Project Term of Reference The MediaWiki servers have to be running on Ubuntu 22.04 OS and must utilize PostgreSQL 14 for data storage, including scheduled db_dump backups for data integrity. Load balancing between the MediaWiki servers has to be managed by an Nginx proxy server to distribute incoming traffic efficiently. System monitoring needs to be performed using Zabbix , overseeing server performance metrics such as CPU, memory, disk usage, and database health to ensure system reliability and early issue detection. As this is a pilot implementation, only 40 users within the local network will access the MediaWiki service through its web interface over the HTTP protocol . Project Objectives Infrastructure Design Development of a deployment scheme for the corporate documentation service based on MediaWiki. The scheme must include all key components (servers, databases, load balancers, and auxiliary services) and describe their interactions. Infrastructure Deployment Installation and configuration of MediaWiki, PostgreSQL, and auxiliary services (Nginx, Zabbix, etc.). Failover Testing Conducting system failover testing: verifying system functionality after server shutdowns, recovery from backups, and data replication checks.","title":"1. Introduction"},{"location":"#launching-a-corporate-documentation-management-service-using-mediawiki","text":"Dmitry Kirsanov | GitHub The project involves the deployment of a corporate documentation management service using the MediaWiki application.","title":"Launching a corporate documentation management service using MediaWiki"},{"location":"#initial-project-term-of-reference","text":"The MediaWiki servers have to be running on Ubuntu 22.04 OS and must utilize PostgreSQL 14 for data storage, including scheduled db_dump backups for data integrity. Load balancing between the MediaWiki servers has to be managed by an Nginx proxy server to distribute incoming traffic efficiently. System monitoring needs to be performed using Zabbix , overseeing server performance metrics such as CPU, memory, disk usage, and database health to ensure system reliability and early issue detection. As this is a pilot implementation, only 40 users within the local network will access the MediaWiki service through its web interface over the HTTP protocol .","title":"Initial Project Term of Reference"},{"location":"#project-objectives","text":"","title":"Project Objectives"},{"location":"#infrastructure-design","text":"Development of a deployment scheme for the corporate documentation service based on MediaWiki. The scheme must include all key components (servers, databases, load balancers, and auxiliary services) and describe their interactions.","title":"Infrastructure Design"},{"location":"#infrastructure-deployment","text":"Installation and configuration of MediaWiki, PostgreSQL, and auxiliary services (Nginx, Zabbix, etc.).","title":"Infrastructure Deployment"},{"location":"#failover-testing","text":"Conducting system failover testing: verifying system functionality after server shutdowns, recovery from backups, and data replication checks.","title":"Failover Testing"},{"location":"2.%20app_deploy_schema_v4/","text":"Application deployment schema Components VM-0 vm-0-service-virtual-machine \u2014 Service VM for Administration and Deployment Stack: Alpine Linux v3.20, Docker, GitHub, Terraform, Ansible, Python. Show description The administrator uses Docker containers and a GitHub repository for the automated deployment, management, and execution of Python scripts on a service VM. The VM serves as an entry point for managing the entire system. VM-1 vm-1-monitoring-system + VHDD-1 vhdd-1-monitoring-system-db \u2014 Monitoring System (Zabbix + PostgreSQL) + External HDD drive. Stack: Ubuntu 22.04, Zabbix-Server, PostgreSQL. Show description The monitoring system is responsible for overseeing the state of all infrastructure components. The Zabbix server collects and analyzes data from the servers, while PostgreSQL stores the monitoring information. Data is written to a mounted hard disk (VHDD-1) vhdd-1-monitoring-system-db to prevent data loss in case of a system failure. VM-2 vm-2-nginx-proxy-server \u2014 Proxy Server. User Requests to MediaWiki Servers Stack: Ubuntu 22.04, Nginx, PostgreSQL. Show description The Nginx proxy server distributes the load between the MediaWiki servers ( VM-3 vm-3-mediawiki-server-1 and VM-4 vm-4-mediawiki-server-2 ) to ensure the smooth operation of the service. VM-3 vm-3-mediawiki-server-1 , VM-4 vm-4-mediawiki-server-2 \u2014 MediaWiki servers Stack: Ubuntu 22.04, MediaWiki, Zabbix-agent. Show description The MediaWiki servers handle user requests and read from and write data to the PostgreSQL database. VM-5 vm-5-haproxy-proxy-server \u2014 Proxy Server. MediaWiki Requests to PostgreSQL db Stack: Ubuntu 22.04, HAProxy, Zabbix-agent. Show description The HAProxy proxy server is responsible for distributing requests from the MediaWiki servers between the Primary PostgreSQL vm-6-primary-db and Standby PostgreSQLL vm-7-standby-db databases. VM-6 vm-6-primary-db + VSSD-1 vssd-1-primary-db \u2014 Primary PostgreSQL db + External SSD-drive Stack: Ubuntu 22.04, PostgreSQL, Zabbix-agent. Show description The Primary PostgreSQL vm-6-primary-db handles read/write requests coming through HAProxy proxy server vm-5-haproxy-proxy-server . The data is stored on a dedicated VSSD-1 vssd-1-primary-db to enhance the speed of data processing. VM-7 vm-7-standby-db + VHDD-2 vhdd-2-standby-db + VHDD-3 vhdd-3-dump-db \u2014 Standby PostgreSQL db. Replication from the Primary db and pg_dump backup + 2 External HDD drives (replication data storage and backups) Stack: Ubuntu 22.04, PostgreSQL, Zabbix-agent. Show description The Standby PostgreSQL db vm-7-standby-db performs asynchronous data replication from the Primary PostgreSQL db vm-6-primary-db to VHDD-2 vhdd-2-standby-db . This allows for a quick failover in case the Primary PostgreSQL db vm-6-primary-db fails. The pg_dump utility is used for backups on VHDD-3 vhdd-3-dump-db . This enables restoring the database to a specific point in time, which can be useful if the database has been compromised by malware that has already replicated to both databases. Visualisation Download the .drawio-file Download the .drawio-file","title":"2. App deployment schema"},{"location":"2.%20app_deploy_schema_v4/#application-deployment-schema","text":"","title":"Application deployment schema"},{"location":"2.%20app_deploy_schema_v4/#components","text":"VM-0 vm-0-service-virtual-machine \u2014 Service VM for Administration and Deployment Stack: Alpine Linux v3.20, Docker, GitHub, Terraform, Ansible, Python. Show description The administrator uses Docker containers and a GitHub repository for the automated deployment, management, and execution of Python scripts on a service VM. The VM serves as an entry point for managing the entire system. VM-1 vm-1-monitoring-system + VHDD-1 vhdd-1-monitoring-system-db \u2014 Monitoring System (Zabbix + PostgreSQL) + External HDD drive. Stack: Ubuntu 22.04, Zabbix-Server, PostgreSQL. Show description The monitoring system is responsible for overseeing the state of all infrastructure components. The Zabbix server collects and analyzes data from the servers, while PostgreSQL stores the monitoring information. Data is written to a mounted hard disk (VHDD-1) vhdd-1-monitoring-system-db to prevent data loss in case of a system failure. VM-2 vm-2-nginx-proxy-server \u2014 Proxy Server. User Requests to MediaWiki Servers Stack: Ubuntu 22.04, Nginx, PostgreSQL. Show description The Nginx proxy server distributes the load between the MediaWiki servers ( VM-3 vm-3-mediawiki-server-1 and VM-4 vm-4-mediawiki-server-2 ) to ensure the smooth operation of the service. VM-3 vm-3-mediawiki-server-1 , VM-4 vm-4-mediawiki-server-2 \u2014 MediaWiki servers Stack: Ubuntu 22.04, MediaWiki, Zabbix-agent. Show description The MediaWiki servers handle user requests and read from and write data to the PostgreSQL database. VM-5 vm-5-haproxy-proxy-server \u2014 Proxy Server. MediaWiki Requests to PostgreSQL db Stack: Ubuntu 22.04, HAProxy, Zabbix-agent. Show description The HAProxy proxy server is responsible for distributing requests from the MediaWiki servers between the Primary PostgreSQL vm-6-primary-db and Standby PostgreSQLL vm-7-standby-db databases. VM-6 vm-6-primary-db + VSSD-1 vssd-1-primary-db \u2014 Primary PostgreSQL db + External SSD-drive Stack: Ubuntu 22.04, PostgreSQL, Zabbix-agent. Show description The Primary PostgreSQL vm-6-primary-db handles read/write requests coming through HAProxy proxy server vm-5-haproxy-proxy-server . The data is stored on a dedicated VSSD-1 vssd-1-primary-db to enhance the speed of data processing. VM-7 vm-7-standby-db + VHDD-2 vhdd-2-standby-db + VHDD-3 vhdd-3-dump-db \u2014 Standby PostgreSQL db. Replication from the Primary db and pg_dump backup + 2 External HDD drives (replication data storage and backups) Stack: Ubuntu 22.04, PostgreSQL, Zabbix-agent. Show description The Standby PostgreSQL db vm-7-standby-db performs asynchronous data replication from the Primary PostgreSQL db vm-6-primary-db to VHDD-2 vhdd-2-standby-db . This allows for a quick failover in case the Primary PostgreSQL db vm-6-primary-db fails. The pg_dump utility is used for backups on VHDD-3 vhdd-3-dump-db . This enables restoring the database to a specific point in time, which can be useful if the database has been compromised by malware that has already replicated to both databases.","title":"Components"},{"location":"2.%20app_deploy_schema_v4/#visualisation","text":"","title":"Visualisation"},{"location":"2.%20app_deploy_schema_v4/#download-the-drawio-file","text":"Download the .drawio-file","title":"Download the .drawio-file"},{"location":"3.1.%20service_vm_docker_setup/","text":"Service VM Docker Configuration Download and Install Docker-desktop Install VScode Docker extension Create Dockerfile ( GitHub ) Show Dockerfile # Using the Alpine Linux base image FROM alpine:latest # Updating packages and installing dependencies RUN apk update && apk add --no-cache \\ bash \\ bash-completion \\ curl \\ wget \\ git \\ unzip \\ python3 \\ py3-pip \\ gnupg \\ ca-certificates \\ sudo \\ openssh \\ sshpass \\ ansible # Generating SSH keys RUN ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519 -N \"\" # Installing Terraform RUN wget https://releases.hashicorp.com/terraform/1.5.7/terraform_1.5.7_linux_amd64.zip && \\ unzip terraform_1.5.7_linux_amd64.zip && \\ mv terraform /usr/local/bin/ && \\ rm terraform_1.5.7_linux_amd64.zip # Installing Yandex Cloud CLI RUN curl -sSL https://storage.yandexcloud.net/yandexcloud-yc/install.sh | bash # Copying Yandex Cloud CLI binary files to /usr/bin/ RUN cp -r ~/yandex-cloud/bin/* /usr/bin/ # Activating bash-completion RUN echo \"source /usr/share/bash-completion/bash_completion\" >> ~/.bashrc # Setting bash as the default shell. CMD [\"/bin/bash\"] Running a previously downloaded Dockerfile to create an Alpine Linux OS image with the required packages and dependencies # - docker build - create Docker-image # - -t mediawiki_service_alpine - arbitrary Docker-image name # - . - build context (where to look for the Dockerfile). In this case, it refers to the current directory docker build -t mediawiki_service_alpine . Start a Docker-container using the previously created Docker-image ( \"Alpine Linux:latest\" ) # - --hostname <hostname> - arbitrary VM hostname # - --name <Docker-container name> - arbitrary Docker-container name # - it <Docker-image name> - Docker-image name used for Docker-container building # - bash - shell docker run --hostname vm-0-service --name mediawiki_service_alpine-container -it mediawiki_service_alpine bash Attaching a Docker container to the VSCode workspace for convenient work Clone the Git repository to the VM-0 vm-0-service-virtual-machine (into the ~ directory). Create a Python virtual environment in ~/repository_name on the VM-0 vm-0-service-virtual-machine # Create a Python virtual environment python3 -m venv pyvenv # Activate a Python virtual environment source pyvenv/bin/activate # Upgrade pip python3 -m pip install --upgrade pip # Install requirements pip install -r python_scripts/requirements.txt # Commit changes when adding additional pip packages pip freeze > python_scripts/requirements.txt","title":"3. Service VM Docker Configuration"},{"location":"3.1.%20service_vm_docker_setup/#service-vm-docker-configuration","text":"Download and Install Docker-desktop Install VScode Docker extension Create Dockerfile ( GitHub ) Show Dockerfile # Using the Alpine Linux base image FROM alpine:latest # Updating packages and installing dependencies RUN apk update && apk add --no-cache \\ bash \\ bash-completion \\ curl \\ wget \\ git \\ unzip \\ python3 \\ py3-pip \\ gnupg \\ ca-certificates \\ sudo \\ openssh \\ sshpass \\ ansible # Generating SSH keys RUN ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519 -N \"\" # Installing Terraform RUN wget https://releases.hashicorp.com/terraform/1.5.7/terraform_1.5.7_linux_amd64.zip && \\ unzip terraform_1.5.7_linux_amd64.zip && \\ mv terraform /usr/local/bin/ && \\ rm terraform_1.5.7_linux_amd64.zip # Installing Yandex Cloud CLI RUN curl -sSL https://storage.yandexcloud.net/yandexcloud-yc/install.sh | bash # Copying Yandex Cloud CLI binary files to /usr/bin/ RUN cp -r ~/yandex-cloud/bin/* /usr/bin/ # Activating bash-completion RUN echo \"source /usr/share/bash-completion/bash_completion\" >> ~/.bashrc # Setting bash as the default shell. CMD [\"/bin/bash\"] Running a previously downloaded Dockerfile to create an Alpine Linux OS image with the required packages and dependencies # - docker build - create Docker-image # - -t mediawiki_service_alpine - arbitrary Docker-image name # - . - build context (where to look for the Dockerfile). In this case, it refers to the current directory docker build -t mediawiki_service_alpine . Start a Docker-container using the previously created Docker-image ( \"Alpine Linux:latest\" ) # - --hostname <hostname> - arbitrary VM hostname # - --name <Docker-container name> - arbitrary Docker-container name # - it <Docker-image name> - Docker-image name used for Docker-container building # - bash - shell docker run --hostname vm-0-service --name mediawiki_service_alpine-container -it mediawiki_service_alpine bash Attaching a Docker container to the VSCode workspace for convenient work Clone the Git repository to the VM-0 vm-0-service-virtual-machine (into the ~ directory). Create a Python virtual environment in ~/repository_name on the VM-0 vm-0-service-virtual-machine # Create a Python virtual environment python3 -m venv pyvenv # Activate a Python virtual environment source pyvenv/bin/activate # Upgrade pip python3 -m pip install --upgrade pip # Install requirements pip install -r python_scripts/requirements.txt # Commit changes when adding additional pip packages pip freeze > python_scripts/requirements.txt","title":"Service VM Docker Configuration"},{"location":"4.1.%20yandex_cloud_cli_and_serv_acc_setup/","text":"Yandex Cloud CLI profile and Service Account Setup Create yc_meta.json file with authentication data in ~/repository_name/credentials directory service_account_id , cloud-id , folder-id , profile-name Show yc_meta_EXAMPLE.json { \"service_account_id\": \"sadsdsdsd...\", \"cloud-id\": \"asdsadasd.....\", \"folder-id\": \"dadsad.....\", \"profile-name\": \"test_name\" } Create a Yandex Cloud CLI profile (if not already created) # Verify the Yandex Cloud CLI installation and profile configuration yc config list Add environment variables to ~/.bashrc user-specific configuration file for the Bash shell by running add_env_var.py ~/repository_name/python_scripts/add_env_var.py After running the script, you must restart the terminal Show .bashrc_EXAMPLE source /usr/share/bash-completion/bash_completion export REPO_NAME=\"repository_name\" export REPO_RELATIVE_PATH=\"~/repository_name\" export REPO_PATH=\"/username/repository_name\" export TERRAFORM_FOLDER_NAME=\"Terraform_MediaWiki\" export TERRAFORM_RELATIVE_PATH=\"~/repository_name/Terraform_MediaWiki\" export TERRAFORM_ABSOLUTE_PATH=\"/username/repository_name/Terraform_MediaWiki\" export ANSIBLE_DIR_NAME=\"Ansible\" export ANSIBLE_DIR_RELATIVE_PATH=\"~/repository_name/Ansible\" export ANSIBLE_DIR_ABSOLUTE_PATH=\"/username/repository_name/Ansible\" export PYTHON_SCRIPTS_DIR_NAME=\"python_scripts\" export PYTHON_SCRIPTS_DIR_RELATIVE_PATH=\"~/repository_name/python_scripts\" export PYTHON_SCRIPTS_DIR_ABSOLUTE_PATH=\"/username/repository_name/python_scripts\" export CREDENTIALS_DIR_NAME=\"credentials\" export CREDENTIALS_DIR_RELATIVE_PATH=\"~/repository_name/credentials\" export CREDENTIALS_DIR_ABSOLUTE_PATH=\"/username/repository_name/credentials\" export TF_VAR_TERRAFORM_META_DIR_ABSOLUTE_PATH=\"/username/repository_name/credentials/terraform_meta.txt\" export YC_TOKEN=\"$(yc iam create-token)\" export YC_CLOUD_ID=\"$(yc config get cloud-id)\" export YC_FOLDER_ID=\"$(yc config get folder-id)\" Set up Yandex Cloud service account configuration by running yc_service_account_configuration.py ~/repository_name/python_scripts/yc_service_account_configuration.py Create and configure a local Yandex Cloud (yc) profile, and automatically generate the yc_meta.json ~/repository_name/credentials/yc_meta.json file with authentication data.","title":"4. Yandex Cloud CLI profile and Service Account Setup"},{"location":"4.1.%20yandex_cloud_cli_and_serv_acc_setup/#yandex-cloud-cli-profile-and-service-account-setup","text":"Create yc_meta.json file with authentication data in ~/repository_name/credentials directory service_account_id , cloud-id , folder-id , profile-name Show yc_meta_EXAMPLE.json { \"service_account_id\": \"sadsdsdsd...\", \"cloud-id\": \"asdsadasd.....\", \"folder-id\": \"dadsad.....\", \"profile-name\": \"test_name\" } Create a Yandex Cloud CLI profile (if not already created) # Verify the Yandex Cloud CLI installation and profile configuration yc config list Add environment variables to ~/.bashrc user-specific configuration file for the Bash shell by running add_env_var.py ~/repository_name/python_scripts/add_env_var.py After running the script, you must restart the terminal Show .bashrc_EXAMPLE source /usr/share/bash-completion/bash_completion export REPO_NAME=\"repository_name\" export REPO_RELATIVE_PATH=\"~/repository_name\" export REPO_PATH=\"/username/repository_name\" export TERRAFORM_FOLDER_NAME=\"Terraform_MediaWiki\" export TERRAFORM_RELATIVE_PATH=\"~/repository_name/Terraform_MediaWiki\" export TERRAFORM_ABSOLUTE_PATH=\"/username/repository_name/Terraform_MediaWiki\" export ANSIBLE_DIR_NAME=\"Ansible\" export ANSIBLE_DIR_RELATIVE_PATH=\"~/repository_name/Ansible\" export ANSIBLE_DIR_ABSOLUTE_PATH=\"/username/repository_name/Ansible\" export PYTHON_SCRIPTS_DIR_NAME=\"python_scripts\" export PYTHON_SCRIPTS_DIR_RELATIVE_PATH=\"~/repository_name/python_scripts\" export PYTHON_SCRIPTS_DIR_ABSOLUTE_PATH=\"/username/repository_name/python_scripts\" export CREDENTIALS_DIR_NAME=\"credentials\" export CREDENTIALS_DIR_RELATIVE_PATH=\"~/repository_name/credentials\" export CREDENTIALS_DIR_ABSOLUTE_PATH=\"/username/repository_name/credentials\" export TF_VAR_TERRAFORM_META_DIR_ABSOLUTE_PATH=\"/username/repository_name/credentials/terraform_meta.txt\" export YC_TOKEN=\"$(yc iam create-token)\" export YC_CLOUD_ID=\"$(yc config get cloud-id)\" export YC_FOLDER_ID=\"$(yc config get folder-id)\" Set up Yandex Cloud service account configuration by running yc_service_account_configuration.py ~/repository_name/python_scripts/yc_service_account_configuration.py Create and configure a local Yandex Cloud (yc) profile, and automatically generate the yc_meta.json ~/repository_name/credentials/yc_meta.json file with authentication data.","title":"Yandex Cloud CLI profile and Service Account Setup"},{"location":"5.1.%20yandex_cloud_terraform_setup/","text":"Yandex Cloud Terraform Setup Create .terraformrc provider configuration file in ~/ directory Automatic Yandex Cloud Terraform provider installation by running terraform_init.py ~/repository_name/python_scripts/terraform_init.py The main.tf ~/repository_name/Terraform_MediaWiki/main.tf , output.tf ~/repository_name/Terraform_MediaWiki/output.tf , providers.tf ~/repository_name/Terraform_MediaWiki/providers.tf , and terraform.tfstate ~/repository_name/Terraform_MediaWiki/terraform.tfstate files are already configured. No changes are needed Automatic authentication data file (terraform_meta.txt) ~/repository_name/credentials/terraform_meta.txt creation by running update_terraform_meta.py ~/repository_name/python_scripts/update_terraform_meta.py Files with public and private SSH keys are automatically created in the ~/.ssh folder during the image build and when a new container is launched If you need to use the same keys as on another already deployed VM, you must manually copy them from that VM to the new one and run the script Essential Terraform commands (execute in Terraform core folder) # Syntax check of all .tf files terraform validate # Planning and reviewing what Terraform will do terraform plan # Getting started and deploying with Terraform terraform apply -auto-approve # Synchronizing the state of resources with the cloud provider (the terraform.tfstate file will be updated) terraform apply -refresh-only # Deleting all created resources terraform destroy -auto-approve # Retrieving the list of VMs yc compute instance list # Stopping the specified VM yc compute instance stop --id <instance-id> # Mark a resource as 'tainted' for subsequent recreation terraform taint 'yandex_compute_instance.group<GROUP NUMBER>[\"vm-<VM NUMBER>\"]'","title":"5. Yandex Cloud Terraform Setup"},{"location":"5.1.%20yandex_cloud_terraform_setup/#yandex-cloud-terraform-setup","text":"Create .terraformrc provider configuration file in ~/ directory Automatic Yandex Cloud Terraform provider installation by running terraform_init.py ~/repository_name/python_scripts/terraform_init.py The main.tf ~/repository_name/Terraform_MediaWiki/main.tf , output.tf ~/repository_name/Terraform_MediaWiki/output.tf , providers.tf ~/repository_name/Terraform_MediaWiki/providers.tf , and terraform.tfstate ~/repository_name/Terraform_MediaWiki/terraform.tfstate files are already configured. No changes are needed Automatic authentication data file (terraform_meta.txt) ~/repository_name/credentials/terraform_meta.txt creation by running update_terraform_meta.py ~/repository_name/python_scripts/update_terraform_meta.py Files with public and private SSH keys are automatically created in the ~/.ssh folder during the image build and when a new container is launched If you need to use the same keys as on another already deployed VM, you must manually copy them from that VM to the new one and run the script Essential Terraform commands (execute in Terraform core folder) # Syntax check of all .tf files terraform validate # Planning and reviewing what Terraform will do terraform plan # Getting started and deploying with Terraform terraform apply -auto-approve # Synchronizing the state of resources with the cloud provider (the terraform.tfstate file will be updated) terraform apply -refresh-only # Deleting all created resources terraform destroy -auto-approve # Retrieving the list of VMs yc compute instance list # Stopping the specified VM yc compute instance stop --id <instance-id> # Mark a resource as 'tainted' for subsequent recreation terraform taint 'yandex_compute_instance.group<GROUP NUMBER>[\"vm-<VM NUMBER>\"]'","title":"Yandex Cloud Terraform Setup"},{"location":"6.1.%20ansible_setup/","text":"Ansible setup Create .terraformrc provider configuration file in ~/ directory Automatic Yandex Cloud Terraform provider installation by running terraform_init.py ~/repository_name/python_scripts/terraform_init.py The main.tf ~/repository_name/Terraform_MediaWiki/main.tf , output.tf ~/repository_name/Terraform_MediaWiki/output.tf , providers.tf ~/repository_name/Terraform_MediaWiki/providers.tf , and terraform.tfstate ~/repository_name/Terraform_MediaWiki/terraform.tfstate files are already configured. No changes are needed Automatic authentication data file (terraform_meta.txt) ~/repository_name/credentials/terraform_meta.txt creation by running update_terraform_meta.py ~/repository_name/python_scripts/update_terraform_meta.py Files with public and private SSH keys are automatically created in the ~/.ssh folder during the image build and when a new container is launched If you need to use the same keys as on another already deployed VM, you must manually copy them from that VM to the new one and run the script","title":"6.1. ansible setup"},{"location":"6.1.%20ansible_setup/#ansible-setup","text":"Create .terraformrc provider configuration file in ~/ directory Automatic Yandex Cloud Terraform provider installation by running terraform_init.py ~/repository_name/python_scripts/terraform_init.py The main.tf ~/repository_name/Terraform_MediaWiki/main.tf , output.tf ~/repository_name/Terraform_MediaWiki/output.tf , providers.tf ~/repository_name/Terraform_MediaWiki/providers.tf , and terraform.tfstate ~/repository_name/Terraform_MediaWiki/terraform.tfstate files are already configured. No changes are needed Automatic authentication data file (terraform_meta.txt) ~/repository_name/credentials/terraform_meta.txt creation by running update_terraform_meta.py ~/repository_name/python_scripts/update_terraform_meta.py Files with public and private SSH keys are automatically created in the ~/.ssh folder during the image build and when a new container is launched If you need to use the same keys as on another already deployed VM, you must manually copy them from that VM to the new one and run the script","title":"Ansible setup"}]}